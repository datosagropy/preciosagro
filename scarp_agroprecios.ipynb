{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaWX4iO6AbMkS46/jkRKGV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datosagropy/preciosagro/blob/main/scarp_agroprecios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "VNJJzqRlEUSc",
        "outputId": "9ea3bbef-aaec-4696-f66a-b35e0829937f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.stock.com.py', port=443): Read timed out. (read timeout=10)\")': /category/488-limpieza-hogar-panosfranelarejillas.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.stock.com.py', port=443): Read timed out. (read timeout=10)\")': /category/701-perecedero-panaderia-pan-arabe.aspx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• stock       :    560 filas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/488-limpieza-hogar-panosfranelarejillas.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/506-limpieza-ropa-detergente-en-polvo.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/698-perecedero-panaderia-galleta-molida.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/701-perecedero-panaderia-pan-arabe.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/718-perecedero-pastas-tapas-para-empanadas.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/849-cereales-semillas.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/506-limpieza-ropa-detergente-en-polvo.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/698-perecedero-panaderia-galleta-molida.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/849-cereales-semillas.aspx\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.superseis.com.py', port=443): Read timed out. (read timeout=10)\")': /category/686-perecedero-lacteos-leches-larga-vida.aspx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• superseis   :    460 filas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /almacen/reposteria/cremas-polvo-hornear\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /pastas-frescas/disco-empanadas/hojaldre\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• salemma     :   1276 filas\n",
            "• arete       :     88 filas\n",
            "• losjardines :     40 filas\n",
            "• biggie      :   2252 filas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-1600390159.py:496: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  base = pd.concat([df_prev, df_all], ignore_index=True, sort=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Hoja actualizada: 3466 filas totales\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Scraper unificado de precios – Py 3.7+\n",
        "Revisión: 2025-06-27\n",
        "Clasificación 2 niveles + precios corregidos para Stock y Superseis\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import re\n",
        "import unicodedata\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import List, Dict, Callable, Set, Tuple\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# ───────── Instalación dinámica (solo Colab) ────────────────────────────────\n",
        "if \"google.colab\" in sys.modules:\n",
        "    get_ipython()  # type: ignore\n",
        "    !pip install -q requests pandas beautifulsoup4 gspread gspread_dataframe PyDrive typing_extensions unidecode\n",
        "    from google.colab import drive, auth\n",
        "    drive.mount(\"/content/drive\")\n",
        "    auth.authenticate_user()\n",
        "    BASE_DIR = \"/content/drive/My Drive/preciosfrutihort\"\n",
        "else:\n",
        "    BASE_DIR = os.path.expanduser(\"~/preciosfrutihort\")\n",
        "\n",
        "# ───────────────────────── Constantes ──────────────────────────────────────\n",
        "FILE_TAG        = \"frutihort\"\n",
        "OUT_DIR         = BASE_DIR\n",
        "PATTERN_DAILY   = os.path.join(OUT_DIR, f\"*canasta_{FILE_TAG}_*.csv\")\n",
        "CREDS_JSON      = os.path.join(BASE_DIR, \"cosmic-ascent-464210-p8-c5c205253ac8.json\")\n",
        "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/10zIOm2Ks2vVtg6JH_A9_IHdyAGzcAsN32azbfaxbVnk\"\n",
        "WORKSHEET_NAME  = \"precios_supermercados\"\n",
        "\n",
        "MAX_WORKERS, REQ_TIMEOUT = 8, 10\n",
        "KEY_COLS = [\"Supermercado\", \"CategoríaURL\", \"Producto\", \"FechaConsulta\"]\n",
        "\n",
        "# ────────────────── 1. Normalización y tokenización ──────────────────────\n",
        "def strip_accents(txt: str) -> str:\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", txt)\n",
        "                   if unicodedata.category(c) != \"Mn\")\n",
        "\n",
        "_token_re = re.compile(r\"[a-záéíóúñü]+\", re.I)\n",
        "def tokenize(txt: str) -> List[str]:\n",
        "    return [strip_accents(t.lower()) for t in _token_re.findall(txt)]\n",
        "\n",
        "# ────────────────── 2. Mapas de clasificación (2 niveles) ─────────────────\n",
        "FRUTAS = {\n",
        "    \"naranjas\": {\"naranja\", \"naranjas\"},\n",
        "    \"manzanas\": {\"manzana\", \"manzanas\"},\n",
        "    \"limones\": {\"limon\", \"limones\"},\n",
        "    \"peras\": {\"pera\", \"peras\"},\n",
        "    \"bananas\": {\"banana\", \"bananas\", \"platano\", \"platanos\"},\n",
        "    \"uvas\": {\"uva\", \"uvas\"},\n",
        "    \"frutillas\": {\"frutilla\", \"frutillas\", \"fresa\", \"fresas\"},\n",
        "    \"mangos\": {\"mango\", \"mangos\"},\n",
        "    \"sandías\": {\"sandia\", \"sandias\"},\n",
        "}\n",
        "VERDURAS = {\n",
        "    \"cebollas\": {\"cebolla\", \"cebollas\"},\n",
        "    \"zanahorias\": {\"zanahoria\", \"zanahorias\"},\n",
        "    \"papas\": {\"papa\", \"papas\", \"patata\", \"patatas\"},\n",
        "    \"tomates\": {\"tomate\", \"tomates\"},\n",
        "    \"lechugas\": {\"lechuga\", \"lechugas\"},\n",
        "    \"zapallos\": {\"zapallo\", \"calabaza\", \"zapallos\", \"calabazas\"},\n",
        "    \"pimientos\": {\"pimiento\", \"pimientos\", \"morron\", \"morrones\"},\n",
        "}\n",
        "HUEVOS = {\n",
        "    \"huevo de gallina\": {\"huevo\", \"huevos\"},\n",
        "    \"huevo de codorniz\": {\"codorniz\"},\n",
        "}\n",
        "LECHES = {\n",
        "    \"leche bebible\": {\"leche\", \"uht\", \"entera\", \"descremada\", \"semidescremada\"},\n",
        "    \"leche polvo\": {\"polvo\"},\n",
        "}\n",
        "QUESOS = {\n",
        "    \"queso paraguay\": {\"paraguay\"},\n",
        "    \"queso mozzarella\": {\"mozzarella\"},\n",
        "    \"queso sandwich\": {\"sandwich\", \"sandwichero\"},\n",
        "    \"queso rallado\": {\"rallado\"},\n",
        "}\n",
        "PANIFICADOS = {\n",
        "    \"pan lactal\": {\"pan\", \"lactal\", \"lacteado\"},\n",
        "    \"tortilla / chipas\": {\"tortilla\", \"chipa\", \"chipas\"},\n",
        "    \"galletas\": {\"galleta\", \"galletitas\"},\n",
        "}\n",
        "CEREALES = {\n",
        "    \"avena\": {\"avena\"},\n",
        "    \"cereal desayuno\": {\"cereal\", \"corn\", \"flakes\", \"muesli\"},\n",
        "}\n",
        "\n",
        "GROUP_DEFS: Dict[str, Dict[str, Set[str]]] = {\n",
        "    \"Frutas\": FRUTAS,\n",
        "    \"Verduras\": VERDURAS,\n",
        "    \"Huevos\": HUEVOS,\n",
        "    \"Leches\": LECHES,\n",
        "    \"Quesos\": QUESOS,\n",
        "    \"Panificados\": PANIFICADOS,\n",
        "    \"Cereales\": CEREALES,\n",
        "}\n",
        "GROUP_TOKENS = {g: set().union(*subs.values()) for g, subs in GROUP_DEFS.items()}\n",
        "KEYWORDS_SUPER = set().union(*GROUP_TOKENS.values())\n",
        "\n",
        "def classify(name: str) -> Tuple[str | None, str | None]:\n",
        "    toks = set(tokenize(name))\n",
        "    for grupo, subs in GROUP_DEFS.items():\n",
        "        if toks & GROUP_TOKENS[grupo]:\n",
        "            for sub, words in subs.items():\n",
        "                if toks & words:\n",
        "                    return grupo, sub\n",
        "            return grupo, \"\"\n",
        "    return None, None\n",
        "\n",
        "# ────────────────── 3. Blacklist no-alimentos ────────────────────────────\n",
        "NON_FOOD = {\n",
        "    \"mg\", \"ml\", \"capsula\", \"capsulas\", \"comprimido\", \"comprimidos\",\n",
        "    \"ovulo\", \"ovulos\", \"gel\", \"shampoo\", \"jabon\", \"jabones\",\n",
        "    \"crema\", \"solar\", \"spf\", \"locion\", \"unguento\", \"spray\", \"ampolla\"\n",
        "}\n",
        "def is_non_food(name: str) -> bool:\n",
        "    return any(tok in NON_FOOD for tok in tokenize(name))\n",
        "\n",
        "# ────────────────── 4. Precio ─────────────────────────────────────────────\n",
        "def norm_price(val) -> float:\n",
        "    if isinstance(val, (int, float)):\n",
        "        return float(val)\n",
        "    txt = re.sub(r\"[^\\d,\\.]\", \"\", str(val)).replace(\".\", \"\").replace(\",\", \".\")\n",
        "    try: return float(txt)\n",
        "    except ValueError: return 0.0\n",
        "\n",
        "def _first_price(node: BeautifulSoup, sels: List[str] | None = None) -> float:\n",
        "    sels = sels or [\n",
        "        \"span.price ins span.amount\",\n",
        "        \"span.price > span.amount\",\n",
        "        \"span.woocommerce-Price-amount\",\n",
        "        \"span.amount\",\n",
        "        \"bdi\",\n",
        "        \"[data-price]\",\n",
        "    ]\n",
        "    for sel in sels:\n",
        "        el = node.select_one(sel)\n",
        "        if el:\n",
        "            p = norm_price(el.get_text() or el.get(\"data-price\", \"\"))\n",
        "            if p > 0:\n",
        "                return p\n",
        "    return 0.0\n",
        "\n",
        "# ────────────────── 5. HTTP session ───────────────────────────────────────\n",
        "def _build_session() -> requests.Session:\n",
        "    retry = Retry(total=3, backoff_factor=1.2,\n",
        "                  status_forcelist=(429,500,502,503,504),\n",
        "                  allowed_methods=(\"GET\",\"HEAD\"))\n",
        "    s = requests.Session()\n",
        "    s.headers[\"User-Agent\"] = (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\"\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    s.mount(\"http://\", adapter)\n",
        "    s.mount(\"https://\", adapter)\n",
        "    return s\n",
        "\n",
        "# ────────────────── 6. Base scraper ──────────────────────────────────────\n",
        "class HtmlSiteScraper:\n",
        "    def __init__(self, name: str, base: str):\n",
        "        self.name = name\n",
        "        self.base_url = base.rstrip(\"/\")\n",
        "        self.session = _build_session()\n",
        "\n",
        "    def category_urls(self) -> List[str]: raise NotImplementedError\n",
        "    def parse_category(self, url: str) -> List[Dict]: raise NotImplementedError\n",
        "\n",
        "    def scrape(self) -> List[Dict]:\n",
        "        urls = self.category_urls()\n",
        "        if not urls: return []\n",
        "        fecha = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        out: List[Dict] = []\n",
        "        with ThreadPoolExecutor(MAX_WORKERS) as pool:\n",
        "            futs = {pool.submit(self.parse_category, u): u for u in urls}\n",
        "            for f in as_completed(futs):\n",
        "                for r in f.result():\n",
        "                    r[\"FechaConsulta\"] = fecha\n",
        "                    out.append(r)\n",
        "        return out\n",
        "\n",
        "    def save_csv(self, rows: List[Dict]) -> None:\n",
        "        if not rows: return\n",
        "        os.makedirs(OUT_DIR, exist_ok=True)\n",
        "        fn = f\"{self.name}_canasta_{FILE_TAG}_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "        pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, fn), index=False)\n",
        "\n",
        "# ────────────────── 7. Stock scraper (precios corregidos) ────────────────\n",
        "class StockScraper(HtmlSiteScraper):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"stock\", \"https://www.stock.com.py\")\n",
        "\n",
        "    def category_urls(self) -> List[str]:\n",
        "        try:\n",
        "            r = self.session.get(self.base_url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except: return []\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        return [\n",
        "            urljoin(self.base_url, a[\"href\"])\n",
        "            for a in soup.select('a[href*=\"/category/\"]')\n",
        "            if any(tok in a[\"href\"].lower() for tok in KEYWORDS_SUPER)\n",
        "        ]\n",
        "\n",
        "    def parse_category(self, url: str) -> List[Dict]:\n",
        "        try:\n",
        "            r = self.session.get(url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except: return []\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "        rows: List[Dict] = []\n",
        "        for p in soup.select(\"div.product-item\"):\n",
        "            nm = p.select_one(\"h2.product-title\")\n",
        "            if not nm: continue\n",
        "            name = nm.get_text(\" \", strip=True)\n",
        "            if is_non_food(name): continue\n",
        "            grupo, sub = classify(name)\n",
        "            if not grupo: continue\n",
        "            # <-- uso de selectores explícitos para precio\n",
        "            precio = _first_price(p, [\"span.price-label\", \"span.price\"])\n",
        "            rows.append({\n",
        "                \"Supermercado\": \"Stock\",\n",
        "                \"CategoríaURL\": url,\n",
        "                \"Producto\": name.upper(),\n",
        "                \"Precio\": precio,\n",
        "                \"Grupo\": grupo,\n",
        "                \"Subgrupo\": sub\n",
        "            })\n",
        "        return rows\n",
        "\n",
        "# ────────────────── 8. Superseis scraper (precios corregidos) ────────────\n",
        "class SuperseisScraper(HtmlSiteScraper):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"superseis\", \"https://www.superseis.com.py\")\n",
        "\n",
        "    def category_urls(self) -> List[str]:\n",
        "        try:\n",
        "            r = self.session.get(self.base_url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except: return []\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        return [\n",
        "            urljoin(self.base_url, a[\"href\"])\n",
        "            for a in soup.select('a.collapsed[href*=\"/category/\"]')\n",
        "            if any(tok in a[\"href\"].lower() for tok in KEYWORDS_SUPER)\n",
        "        ]\n",
        "\n",
        "    def parse_category(self, url: str) -> List[Dict]:\n",
        "        try:\n",
        "            r = self.session.get(url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except: return []\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "        rows: List[Dict] = []\n",
        "        for a in soup.select(\"a.product-title-link\"):\n",
        "            name = a.get_text(\" \", strip=True)\n",
        "            if is_non_food(name): continue\n",
        "            grupo, sub = classify(name)\n",
        "            if not grupo: continue\n",
        "            parent = a.find_parent(\"div\", class_=\"product-item\") or a\n",
        "            # <-- uso de selectores explícitos para precio\n",
        "            precio = _first_price(parent, [\"span.price-label\", \"span.price\"])\n",
        "            rows.append({\n",
        "                \"Supermercado\": \"Superseis\",\n",
        "                \"CategoríaURL\": url,\n",
        "                \"Producto\": name.upper(),\n",
        "                \"Precio\": precio,\n",
        "                \"Grupo\": grupo,\n",
        "                \"Subgrupo\": sub\n",
        "            })\n",
        "        return rows\n",
        "\n",
        "class SalemmaScraper(HtmlSiteScraper):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"salemma\", \"https://www.salemmaonline.com.py\")\n",
        "\n",
        "    def category_urls(self) -> List[str]:\n",
        "        try:\n",
        "            r = self.session.get(self.base_url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except Exception:\n",
        "            return []\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        return [\n",
        "            urljoin(self.base_url, a[\"href\"])\n",
        "            for a in soup.find_all(\"a\", href=True)\n",
        "            if any(tok in a[\"href\"].lower() for tok in KEYWORDS_SUPER)\n",
        "        ]\n",
        "\n",
        "    def parse_category(self, url: str) -> List[Dict]:\n",
        "        try:\n",
        "            r = self.session.get(url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except Exception:\n",
        "            return []\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "        rows: List[Dict] = []\n",
        "        for f in soup.select(\"form.productsListForm\"):\n",
        "            nombre = f.find(\"input\", {\"name\": \"name\"}).get(\"value\", \"\")\n",
        "            if is_non_food(nombre):\n",
        "                continue\n",
        "            grupo, sub = classify(nombre)\n",
        "            if not grupo:\n",
        "                continue\n",
        "            precio = norm_price(f.find(\"input\", {\"name\": \"price\"}).get(\"value\", \"\"))\n",
        "            rows.append({\n",
        "                \"Supermercado\": \"Salemma\",\n",
        "                \"CategoríaURL\": url,\n",
        "                \"Producto\": nombre.upper(),\n",
        "                \"Precio\": precio,\n",
        "                \"Grupo\": grupo,\n",
        "                \"Subgrupo\": sub\n",
        "            })\n",
        "        return rows\n",
        "\n",
        "class AreteScraper(HtmlSiteScraper):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"arete\", \"https://www.arete.com.py\")\n",
        "\n",
        "    def category_urls(self) -> List[str]:\n",
        "        try:\n",
        "            r = self.session.get(self.base_url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except Exception:\n",
        "            return []\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        urls: Set[str] = set()\n",
        "        for sel in (\"#departments-menu\", \"#menu-departments-menu-1\"):\n",
        "            for a in soup.select(f'{sel} a[href^=\"catalogo/\"]'):\n",
        "                href = a[\"href\"].split(\"?\")[0].lower()\n",
        "                if any(tok in href for tok in KEYWORDS_SUPER):\n",
        "                    urls.add(urljoin(self.base_url + \"/\", href))\n",
        "        return list(urls)\n",
        "\n",
        "    def parse_category(self, url: str) -> List[Dict]:\n",
        "        try:\n",
        "            r = self.session.get(url, timeout=REQ_TIMEOUT)\n",
        "            r.raise_for_status()\n",
        "        except Exception:\n",
        "            return []\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "        rows: List[Dict] = []\n",
        "        for p in soup.select(\"div.product\"):\n",
        "            nm = p.select_one(\"h2.ecommercepro-loop-product__title\")\n",
        "            if not nm:\n",
        "                continue\n",
        "            nombre = nm.get_text(\" \", strip=True)\n",
        "            if is_non_food(nombre):\n",
        "                continue\n",
        "            grupo, sub = classify(nombre)\n",
        "            if not grupo:\n",
        "                continue\n",
        "            precio = _first_price(p)\n",
        "            rows.append({\n",
        "                \"Supermercado\": \"Arete\",\n",
        "                \"CategoríaURL\": url,\n",
        "                \"Producto\": nombre.upper(),\n",
        "                \"Precio\": precio,\n",
        "                \"Grupo\": grupo,\n",
        "                \"Subgrupo\": sub\n",
        "            })\n",
        "        return rows\n",
        "\n",
        "class JardinesScraper(AreteScraper):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = \"losjardines\"\n",
        "        self.base_url = \"https://losjardinesonline.com.py\"\n",
        "\n",
        "# ────────────────── 8. Biggie (API) ──────────────────────────────────────\n",
        "class BiggieScraper:\n",
        "    name, API, TAKE = \"biggie\", \"https://api.app.biggie.com.py/api/articles\", 100\n",
        "    GROUPS = [\"frutas-y-verduras\", \"huevos\", \"lacteos\", \"frutas\", \"verduras\"]\n",
        "    session = _build_session()\n",
        "\n",
        "    def fetch_group(self, grp: str) -> List[Dict]:\n",
        "        rows, skip = [], 0\n",
        "        while True:\n",
        "            js = self.session.get(self.API, params={\n",
        "                \"take\": self.TAKE,\n",
        "                \"skip\": skip,\n",
        "                \"classificationName\": grp\n",
        "            }, timeout=REQ_TIMEOUT).json()\n",
        "            for it in js.get(\"items\", []):\n",
        "                nombre = it.get(\"name\", \"\")\n",
        "                if is_non_food(nombre):\n",
        "                    continue\n",
        "                grupo, sub = classify(nombre)\n",
        "                if not grupo:\n",
        "                    continue\n",
        "                rows.append({\n",
        "                    \"Supermercado\": \"Biggie\",\n",
        "                    \"CategoríaURL\": grp,\n",
        "                    \"Producto\": nombre.upper(),\n",
        "                    \"Precio\": norm_price(it.get(\"price\", 0)),\n",
        "                    \"Grupo\": grupo,\n",
        "                    \"Subgrupo\": sub\n",
        "                })\n",
        "            skip += self.TAKE\n",
        "            if skip >= js.get(\"count\", 0):\n",
        "                break\n",
        "        return rows\n",
        "\n",
        "    def scrape(self) -> List[Dict]:\n",
        "        fecha = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        out: List[Dict] = []\n",
        "        for g in self.GROUPS:\n",
        "            for r in self.fetch_group(g):\n",
        "                r[\"FechaConsulta\"] = fecha\n",
        "                out.append(r)\n",
        "        return out\n",
        "\n",
        "    def save_csv(self, rows: List[Dict]) -> None:\n",
        "        if rows:\n",
        "            os.makedirs(OUT_DIR, exist_ok=True)\n",
        "            fn = f\"{self.name}_canasta_{FILE_TAG}_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "            pd.DataFrame(rows).to_csv(os.path.join(OUT_DIR, fn), index=False)\n",
        "\n",
        "# ────────────────── 9. Gestor de scrapers ─────────────────────────────────\n",
        "SCRAPERS: Dict[str, Callable[[], object]] = {\n",
        "    \"stock\": StockScraper,\n",
        "    \"superseis\": SuperseisScraper,\n",
        "    \"salemma\": SalemmaScraper,\n",
        "    \"arete\": AreteScraper,\n",
        "    \"losjardines\": JardinesScraper,\n",
        "    \"biggie\": BiggieScraper,\n",
        "}\n",
        "\n",
        "def _parse_args(argv: List[str] | None = None) -> List[str]:\n",
        "    if argv is None:\n",
        "        return list(SCRAPERS)\n",
        "    if any(a in (\"-h\", \"--help\") for a in argv):\n",
        "        print(\"Uso: python scraper.py [sitio1 sitio2 …]\"); sys.exit(0)\n",
        "    sel = [a for a in argv if a in SCRAPERS]\n",
        "    return sel or list(SCRAPERS)\n",
        "\n",
        "# ────────────────── 10. Google Sheets helpers ─────────────────────────────\n",
        "def _open_sheet():\n",
        "    import gspread\n",
        "    from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "    from google.oauth2.service_account import Credentials\n",
        "\n",
        "    scopes = [\"https://www.googleapis.com/auth/drive\",\n",
        "              \"https://www.googleapis.com/auth/spreadsheets\"]\n",
        "    creds = Credentials.from_service_account_file(CREDS_JSON, scopes=scopes)\n",
        "    sh = gspread.authorize(creds).open_by_url(SPREADSHEET_URL)\n",
        "    try:\n",
        "        ws = sh.worksheet(WORKSHEET_NAME)\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        ws = sh.add_worksheet(title=WORKSHEET_NAME, rows=\"10000\", cols=\"40\")\n",
        "    df = (get_as_dataframe(ws, dtype=str, evaluate_formulas=False)\n",
        "          .dropna(how=\"all\"))\n",
        "    return ws, df\n",
        "\n",
        "def _write_sheet(ws, df: pd.DataFrame) -> None:\n",
        "    from gspread_dataframe import set_with_dataframe\n",
        "    ws.clear()\n",
        "    set_with_dataframe(ws, df, include_index=False)\n",
        "\n",
        "# ────────────────── 11. Orquestador principal ────────────────────────────\n",
        "def main(argv: List[str] | None = None) -> int:\n",
        "    objetivos = _parse_args(argv if argv is not None else sys.argv[1:])\n",
        "    registros: List[Dict] = []\n",
        "    for k in objetivos:\n",
        "        scraper = SCRAPERS[k]()\n",
        "        filas = scraper.scrape()\n",
        "        scraper.save_csv(filas)\n",
        "        registros.extend(filas)\n",
        "        print(f\"• {k:<12}: {len(filas):>6} filas\")\n",
        "    if not registros:\n",
        "        print(\"Sin datos nuevos.\"); return 0\n",
        "\n",
        "    csv_files = glob.glob(PATTERN_DAILY)\n",
        "    if not csv_files:\n",
        "        print(\"⚠️ No se encontraron archivos CSV para concatenar.\"); return 0\n",
        "\n",
        "    df_all = pd.concat([pd.read_csv(f, dtype=str) for f in csv_files],\n",
        "                       ignore_index=True, sort=False)\n",
        "    df_all[\"Precio\"] = pd.to_numeric(df_all[\"Precio\"], errors=\"coerce\")\n",
        "\n",
        "    ws, df_prev = _open_sheet()\n",
        "    base = pd.concat([df_prev, df_all], ignore_index=True, sort=False)\n",
        "    base[\"FechaConsulta\"] = pd.to_datetime(base[\"FechaConsulta\"], errors=\"coerce\")\n",
        "    base.sort_values(\"FechaConsulta\", inplace=True)\n",
        "    base[\"FechaConsulta\"] = base[\"FechaConsulta\"].dt.strftime(\"%Y-%m-%d\")\n",
        "    base.drop_duplicates(KEY_COLS + [\"Subgrupo\"], keep=\"first\", inplace=True)\n",
        "\n",
        "    if \"ID\" in base.columns:\n",
        "        base.drop(columns=[\"ID\"], inplace=True)\n",
        "    base.insert(0, \"ID\", range(1, len(base) + 1))\n",
        "\n",
        "    _write_sheet(ws, base)\n",
        "    print(f\"✅ Hoja actualizada: {len(base)} filas totales\")\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sys.exit(main())\n"
      ]
    }
  ]
}