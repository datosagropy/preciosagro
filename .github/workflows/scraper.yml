name: ðŸ¥• Run Precios-Scraper

# Se ejecuta cada dÃ­a a las 02:00 UTC y tambiÃ©n puede dispararse manualmente
on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  scrape:
    name: Ejecutar Scraper
    runs-on: ubuntu-latest
    env:
      BASE_DIR: ${{ github.workspace }}
      CREDS_JSON: creds.json
      # Mejor poner la URL en Secrets y referenciarla aquÃ­:
      SPREADSHEET_URL: ${{ secrets.SPREADSHEET_URL }}
      WORKSHEET_NAME: precios_supermercados
      FILE_TAG: frutihort

    steps:
      # 1) Clonar tu cÃ³digo
      - name: Checkout repo
        uses: actions/checkout@v3

      # 2) Cachear pip para acelerar futuras ejecuciones
      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      # 3) Instalar Python
      - name: Setup Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      # 4) Instalar dependencias
      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # 5) Escribir credenciales de Google desde el Secret
      - name: Configurar credenciales de Google
        run: |
          echo "${{ secrets.GOOGLE_CREDENTIALS }}" > creds.json

      # 6) Ejecutar tu scraper
      - name: Ejecutar scraper.py
        run: |
          python scraper.py

      # 7) Publicar CSVs como artefacto
      - name: Publicar CSV como artefacto
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: precios-csv
          path: out/**/*.csv
